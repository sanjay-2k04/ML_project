# -*- coding: utf-8 -*-
"""ML PAPER.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jwFTo9zTk9TdXHJjkfMRCmlZGbLtR7Hs

#**KNN**
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import *
from sklearn.naive_bayes import GaussianNB
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.ensemble import *

df1= pd.read_csv("glass.csv")

df1.drop('Index',axis=1,inplace=True)

X = df1.drop(['Class'], axis=1)
y = df1['Class']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

knn= KNeighborsClassifier(n_neighbors=3)

knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
# Calculate and print accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

y_train_pred = knn.predict(X_train)
y_test_pred = knn.predict(X_test)

# Calculate and print accuracy
train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)
print(f"Training Accuracy: {train_accuracy}")
print(f"Testing Accuracy: {test_accuracy}")

# Classification report
print("Training Classification Report:")
print(classification_report(y_train, y_train_pred))

print("Testing Classification Report:")
print(classification_report(y_test, y_test_pred))

import matplotlib.pyplot as plt
import seaborn as sns

accuracies = [train_accuracy, test_accuracy]
labels = ['Training Accuracy', 'Testing Accuracy']

plt.figure(figsize=(10, 6))
sns.barplot(x=labels, y=accuracies, palette='viridis')
plt.ylim(0, 1)  # Ensure the y-axis is between 0 and 1
plt.title('Training and Testing Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Dataset')

# Display the plot
plt.show()

"""#**NAIVE BAYES**"""

from sklearn.naive_bayes import GaussianNB
nb_classifier = GaussianNB()
nb_classifier.fit(X_train, y_train)
y_pred =nb_classifier.predict(X_test)

# Calculate and print accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

y_train_pred = nb_classifier.predict(X_train)
y_test_pred = nb_classifier.predict(X_test)

# Calculate and print accuracy
train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)
print(f"Training Accuracy: {train_accuracy}")
print(f"Testing Accuracy: {test_accuracy}")

# Classification report
print("Training Classification Report:")
print(classification_report(y_train, y_train_pred))

print("Testing Classification Report:")
print(classification_report(y_test, y_test_pred))

y_train_pred = nb_classifier.predict(X_train)
y_test_pred = nb_classifier.predict(X_test)

# Calculate and print accuracy
train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)
print(f"Training Accuracy: {train_accuracy}")
print(f"Testing Accuracy: {test_accuracy}")

# Classification report
print("Training Classification Report:")
print(classification_report(y_train, y_train_pred))

print("Testing Classification Report:")
print(classification_report(y_test, y_test_pred))

import matplotlib.pyplot as plt
import seaborn as sns

accuracies = [train_accuracy, test_accuracy]
labels = ['Training Accuracy', 'Testing Accuracy']

plt.figure(figsize=(10, 6))
sns.barplot(x=labels, y=accuracies, palette='viridis')
plt.ylim(0, 1)  # Ensure the y-axis is between 0 and 1
plt.title('Training and Testing Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Dataset')

# Display the plot
plt.show()

"""#**LOGISTIC REGRESSION**"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
log_reg = LogisticRegression(multi_class='ovr', solver='liblinear')  # OvR strategy with 'liblinear' solver
log_reg.fit(X_train, y_train)

# Make predictions
y_pred = log_reg.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

y_train_pred = log_reg.predict(X_train)
y_test_pred = log_reg.predict(X_test)

# Calculate and print accuracy
train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)
print(f"Training Accuracy: {train_accuracy}")
print(f"Testing Accuracy: {test_accuracy}")

# Classification report
print("Training Classification Report:")
print(classification_report(y_train, y_train_pred))

print("Testing Classification Report:")
print(classification_report(y_test, y_test_pred))



"""#**FEED FORWARD NETWORK**"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
import tensorflow as tf
import numpy as np

x_data = df1.drop(['Class'], axis=1)
y_data = df1['Class']
X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.3, random_state=42)

# Standardize the features using StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize and fit the LabelEncoder to encode target variables
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_test_encoded = label_encoder.transform(y_test)

# Define the neural network model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(len(label_encoder.classes_), activation='softmax')  # Number of classes
])

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Set random seed for reproducibility
np.random.seed(42)

# Train the model using encoded target variables
model.fit(X_train_scaled, y_train_encoded, epochs=100, batch_size=64, validation_split=0.1)

# Evaluate the model on test data
test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test_encoded)
print(f"Test Accuracy: {test_accuracy}")

# Make predictions on training and testing sets
y_train_probs = model.predict(X_train_scaled)
y_test_probs = model.predict(X_test_scaled)

# Convert probabilities to class labels
y_train_pred = np.argmax(y_train_probs, axis=1)
y_test_pred = np.argmax(y_test_probs, axis=1)

# Decode the predictions back to original labels
y_train_pred_decoded = label_encoder.inverse_transform(y_train_pred)
y_test_pred_decoded = label_encoder.inverse_transform(y_test_pred)

from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder

# Encode the target variable 'Type' using LabelEncoder
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)

# Decode the predictions back to original labels
y_train_pred_decoded = label_encoder.inverse_transform(y_train_pred)
y_test_pred_decoded = label_encoder.inverse_transform(y_test_pred)

# Print classification report for training set
print("Classification Report for Training Set:")
print(classification_report(y_train_encoded, y_train_pred))

# Print classification report for testing set
print("Classification Report for Testing Set:")
print(classification_report(y_test, y_test_pred_decoded))

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score

# Assuming you have y_train_encoded, y_test_encoded, y_train_pred, and y_test_pred

# Calculate accuracy
train_accuracy = accuracy_score(y_train_encoded, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred_decoded)

# Print accuracies
print(f"Training Accuracy: {train_accuracy}")
print(f"Testing Accuracy: {test_accuracy}")

# Plotting the accuracies
accuracies = [train_accuracy, test_accuracy]
labels = ['Training Accuracy', 'Testing Accuracy']

plt.figure(figsize=(10, 6))
sns.barplot(x=labels, y=accuracies, palette='viridis')
plt.ylim(0, 1)  # Ensure the y-axis is between 0 and 1
plt.title('Training and Testing Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Dataset')
plt.show()

"""#**RNN**"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, SimpleRNN

# Assuming y_data needs to be mapped from 1-7 to 0-6
y_data_mapped = y_data - 1

# Split the data into training and testing sets with a test size of 30% (0.3)
# random_state is set to 42 for reproducibility
X_train, X_test, y_train, y_test = train_test_split(x_data, y_data_mapped, test_size=0.3, random_state=42)

# Standardize the features using StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize and fit the LabelEncoder to encode target variables
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_test_encoded = label_encoder.transform(y_test)

# Define the neural network model with SimpleRNN layer
model = Sequential([
    SimpleRNN(units=64, activation='relu', input_shape=(X_train_scaled.shape[1], 1)),
    Dense(32, activation='relu'),
    Dense(6, activation='softmax')  # Updated to 6 units for 0-5 classes
])

# Reshape input data for SimpleRNN layer
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Set random seed for reproducibility
import numpy as np
np.random.seed(42)

# Train the model
model.fit(X_train_reshaped, y_train_encoded, epochs=100, batch_size=64, validation_split=0.1)

# Evaluate the model on test data
test_loss, test_accuracy = model.evaluate(X_test_reshaped, y_test_encoded)
print(f"Test Accuracy: {test_accuracy}")

# Make predictions on training and testing sets
y_train_probs = model.predict(X_train_reshaped)
y_test_probs = model.predict(X_test_reshaped)

# Convert probabilities to class labels
y_train_pred = np.argmax(y_train_probs, axis=1)
y_test_pred = np.argmax(y_test_probs, axis=1)

# Decode the predictions back to original labels
y_train_pred_decoded = label_encoder.inverse_transform(y_train_pred)
y_test_pred_decoded = label_encoder.inverse_transform(y_test_pred)

# Print classification report for training set
print("Classification Report for Training Set:")
print(classification_report(y_train, y_train_pred_decoded))

# Print classification report for testing set
print("Classification Report for Testing Set:")
print(classification_report(y_test, y_test_pred_decoded))

# Calculate accuracy
train_accuracy = accuracy_score(y_train_encoded, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred_decoded)

# Print accuracies
print(f"Training Accuracy: {train_accuracy}")
print(f"Testing Accuracy: {test_accuracy}")

# Plotting the accuracies
accuracies = [train_accuracy, test_accuracy]
labels = ['Training Accuracy', 'Testing Accuracy']

plt.figure(figsize=(10, 6))
sns.barplot(x=labels, y=accuracies, palette='viridis')
plt.ylim(0, 1)  # Ensure the y-axis is between 0 and 1
plt.title('Training and Testing Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Dataset')
plt.show()

"""#**ANN**"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Assuming y_data needs to be mapped from 1-7 to 0-6
y_data_mapped = y_data - 1

# Split the data into training and testing sets with a test size of 30% (0.3)
# random_state is set to 42 for reproducibility
X_train, X_test, y_train, y_test = train_test_split(x_data, y_data_mapped, test_size=0.3, random_state=42)

# Standardize the features using StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize and fit the LabelEncoder to encode target variables
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_test_encoded = label_encoder.transform(y_test)

# Define the neural network model
model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),
    Dense(32, activation='relu'),
    Dense(6, activation='softmax')  # Updated to 6 units for 0-5 classes
])

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Set random seed for reproducibility
import numpy as np
np.random.seed(42)

# Train the model
model.fit(X_train_scaled, y_train_encoded, epochs=100, batch_size=64, validation_split=0.1)

# Evaluate the model on test data
test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test_encoded)
print(f"Test Accuracy: {test_accuracy}")

# Make predictions on training and testing sets
y_train_probs = model.predict(X_train_scaled)
y_test_probs = model.predict(X_test_scaled)

# Convert probabilities to class labels
y_train_pred = np.argmax(y_train_probs, axis=1)
y_test_pred = np.argmax(y_test_probs, axis=1)

# Decode the predictions back to original labels
y_train_pred_decoded = label_encoder.inverse_transform(y_train_pred)
y_test_pred_decoded = label_encoder.inverse_transform(y_test_pred)

# Print classification report for training set
print("Classification Report for Training Set:")
print(classification_report(y_train, y_train_pred_decoded))

# Print classification report for testing set
print("Classification Report for Testing Set:")
print(classification_report(y_test, y_test_pred_decoded))

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import LabelEncoder

# Assuming y_train, y_test, y_train_probs, y_test_probs are defined and label_encoder is fitted

# Convert probabilities to class labels
y_train_pred = np.argmax(y_train_probs, axis=1)
y_test_pred = np.argmax(y_test_probs, axis=1)

# Decode the predictions back to original labels
y_train_pred_decoded = label_encoder.inverse_transform(y_train_pred)
y_test_pred_decoded = label_encoder.inverse_transform(y_test_pred)

# Print classification report for training set
print("Classification Report for Training Set:")
print(classification_report(y_train, y_train_pred_decoded))

# Print classification report for testing set
print("Classification Report for Testing Set:")
print(classification_report(y_test, y_test_pred_decoded))

# Calculate accuracy
train_accuracy = accuracy_score(y_train, y_train_pred_decoded)
test_accuracy = accuracy_score(y_test, y_test_pred_decoded)

# Print accuracies
print(f"Training Accuracy: {train_accuracy}")
print(f"Testing Accuracy: {test_accuracy}")

# Plotting the accuracies
accuracies = [train_accuracy, test_accuracy]
labels = ['Training Accuracy', 'Testing Accuracy']

plt.figure(figsize=(10, 6))
sns.barplot(x=labels, y=accuracies, palette='viridis')
plt.ylim(0, 1)  # Ensure the y-axis is between 0 and 1
plt.title('Training and Testing Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Dataset')
plt.show()

"""#**CNN**"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import pandas as pd
import numpy as np

# Assuming y_data needs to be mapped from 1-7 to 0-6
y_data_mapped = y_data - 1

# Split the data into training and testing sets with a test size of 30% (0.3)
# random_state is set to 42 for reproducibility
X_train, X_test, y_train, y_test = train_test_split(x_data, y_data_mapped, test_size=0.3, random_state=42)

# Standardize the features using StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize and fit the LabelEncoder to encode target variables
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_test_encoded = label_encoder.transform(y_test)

# Reshape input data for CNN
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))

# Define the CNN model
model = Sequential([
    Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])),
    MaxPooling1D(pool_size=2),
    Flatten(),
    Dense(64, activation='relu'),
    Dense(6, activation='softmax')  # Updated to 6 units for 0-5 classes
])

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Set random seed for reproducibility
np.random.seed(42)

# Train the model
model.fit(X_train_reshaped, y_train_encoded, epochs=100, batch_size=64, validation_split=0.1)

# Evaluate the model on test data
test_loss, test_accuracy = model.evaluate(X_test_reshaped, y_test_encoded)
print(f"Test Accuracy: {test_accuracy}")

# Make predictions on training and testing sets
y_train_probs = model.predict(X_train_reshaped)
y_test_probs = model.predict(X_test_reshaped)

# Convert probabilities to class labels
y_train_pred = np.argmax(y_train_probs, axis=1)
y_test_pred = np.argmax(y_test_probs, axis=1)

# Decode the predictions back to original labels
y_train_pred_decoded = label_encoder.inverse_transform(y_train_pred)
y_test_pred_decoded = label_encoder.inverse_transform(y_test_pred)

# Print classification report for training set
print("Classification Report for Training Set:")
print(classification_report(y_train, y_train_pred_decoded))

# Print classification report for testing set
print("Classification Report for Testing Set:")
print(classification_report(y_test, y_test_pred_decoded))